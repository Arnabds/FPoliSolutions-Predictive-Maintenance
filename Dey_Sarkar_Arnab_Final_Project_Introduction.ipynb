{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FPoliSolutions, LLC; Asset Monitoring and Predictive Maintenance\n",
    "## Introduction\n",
    "\n",
    "#### Arnab Dey Sarkar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we need to study high frequency data to estimate before hand when some heavy machinary is going to break down.\n",
    "\n",
    "I am doing my research in computational neuroscience and I also want to learn about financial quant related analysis. In both these situation, one thing is definitely common, which is Time series. Although we will learn it well next semester. I thought to do some related work and I found this one. This will help me getting a good practice for asset monitoring.\n",
    "\n",
    "Then I also realized that this is a classification problem and a supervised problem. Hence, it will give me a good practice to classifcation as well as dimentinality reduction to find out the number of effective feature variables as this one has 63 feature variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am very much interested in this project but I will do a second one PPG Customer churn as well for non-credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classification problem so far what I can see. The `X, Z,` and `V` columns are input variables. The `Y` column is the response variable. \n",
    "The output is encoded as: \n",
    "* Y = 1 is a FAILURE\n",
    "* Y=0 is NOT a failure.\n",
    "\n",
    "I didn't have to derive the responses of interest. It was given in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it is a classification problem the linear regression won't be effective hence we have to use logistic regression which is basically the log transformation of the interval $(0,1)$ to $\\Bbb R$ and using the inverse transformation we map a linear regression region within $(0,1)$ so that we can consider the output as a probability and then we classify it $1$ if the probability is $>0.5$ or $0$ 1 if the probability is $<0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the project we observed 'Z07', 'Z08', 'Z09', 'V02', 'V11', 'V27', 'V28', 'V29' are highly skewed so we had to apply log transformation to reduce their effect. From mod_03 we have also seen that the important feature was X01, Z01, Z04 but the statistically significant feature was Intercept, V07, V15. We have also seen a good sepaation with V07, V15 and k2. It didn't work that well with Y as the features are correlated when we are making them uncorrelated with PCA the data can be separated well. We tried to concrete our assumption from EDA and cluster in modeling and further. Finally we saw that if we have interaction feature with uncorrelated pca features, then we can separate the data in the best possible way from what we know till now. At the end we saw SVM and Neural Network is actually giving better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding one model regression moderately well is my major take away. I have tried to understand regression as much as possible without jumping into other big names. Hopefully I will enjoy learning other models well in future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
